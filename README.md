# Emotional-Speech-Synthesis
This work was done for the course DT2119 Speech and Speaker Recognition (2022) at Royal Institute of Technology (KTH), Stockholm.
Read the [report.pdf](report.pdf) for all information about the project.
<br />
<br />
The project aimed to condition a TTS system (Tacotron2 + WaveGlow) on a emotion or a speaker identity using the [IEMOCAP](https://sail.usc.edu/iemocap/) and the [VCTK](https://datashare.ed.ac.uk/handle/10283/2950) datasets. It builds upon the following repositories:<br />
https://github.com/NVIDIA/tacotron2 <br />
https://github.com/NVIDIA/waveglow <br />
https://github.com/resemble-ai/Resemblyzer <br />